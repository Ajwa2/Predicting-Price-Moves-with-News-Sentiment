{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf4c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "sns.set(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10,5)\n",
    "def find_repo_root_with_data(start=Path('.'), max_up=5):\n",
    "    p = start.resolve()\n",
    "    for _ in range(max_up):\n",
    "        if (p / 'Data').exists():\n",
    "            return p\n",
    "        if p.parent == p:\n",
    "            break\n",
    "        p = p.parent\n",
    "    return start.resolve()\n",
    "ROOT = find_repo_root_with_data(Path('.'))\n",
    "DATA_DIR = ROOT / 'Data'\n",
    "candidates = []\n",
    "if DATA_DIR.exists():\n",
    "    candidates = sorted([str(p) for p in DATA_DIR.glob('*.csv')])\n",
    "else:\n",
    "\n",
    "    candidates = sorted([str(p) for p in ROOT.rglob('*.csv')])\n",
    "print('Repository root detected:', ROOT)\n",
    "print('Found candidate CSV files:', len(candidates))\n",
    "for c in candidates[:20]:\n",
    "    print(' -', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057dc5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inspect_csv(path, nrows=2000):\n",
    "    print(f'Inspecting {path} (sampling up to {nrows} rows)...')\n",
    "    try:\n",
    "        sample = pd.read_csv(path, nrows=nrows)\n",
    "    except Exception as e:\n",
    "        print('Failed to read sample:', e)\n",
    "        return None\n",
    "    print('Columns:', list(sample.columns))\n",
    "    print('Preview:')\n",
    "    display(sample.head(3))\n",
    "    print('Dtypes:')\n",
    "    display(sample.dtypes)\n",
    "    return sample\n",
    "sample_df = None\n",
    "if candidates:\n",
    "    sample_df = inspect_csv(candidates[0])\n",
    "else:\n",
    "    print('No CSV files found to inspect.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f46b42f",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (Task 1)\n",
    "This notebook performs a defensive EDA on news/text CSVs inside the `Data/` folder. It samples files where possible to avoid loading very large files into memory. The notebook automatically detects likely text columns (e.g. `title`, `body`, `text`) and a date column if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2560964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if sample_df is None and candidates:\n",
    "    sample_df = inspect_csv(candidates[0], nrows=5000)\n",
    "\n",
    "def detect_text_columns(df, min_unique=10):\n",
    "    text_cols = []\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            nunique = df[c].nunique(dropna=True)\n",
    "            if nunique >= min_unique:\n",
    "                text_cols.append(c)\n",
    "    return text_cols\n",
    "\n",
    "text_cols = detect_text_columns(sample_df) if sample_df is not None else []\n",
    "print('Detected text-like columns:', text_cols)\n",
    "\n",
    "possible_date_cols = [c for c in (sample_df.columns if sample_df is not None else []) if 'date' in c.lower() or 'time' in c.lower() or 'publish' in c.lower()]\n",
    "print('Detected possible date columns:', possible_date_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6147856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if sample_df is not None and text_cols:\n",
    "    col = text_cols[0]\n",
    "    s = sample_df[col].astype(str)\n",
    "    lengths = s.str.len().fillna(0)\n",
    "    print('Count:', len(s))\n",
    "    print('Missing:', s.isna().sum())\n",
    "    print('Length stats:')\n",
    "    display(lengths.describe())\n",
    "    sns.histplot(lengths, bins=40);\n",
    "else:\n",
    "    print('No text columns available in sample to compute descriptive stats.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab5942",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "stops = list(stopwords.words('english'))\n",
    "\n",
    "if sample_df is not None and text_cols:\n",
    "    texts = sample_df[text_cols[0]].fillna('').astype(str).values[:5000]\n",
    "    cv = CountVectorizer(stop_words=stops, max_features=5000, ngram_range=(1,1))\n",
    "    X = cv.fit_transform(texts)\n",
    "    freqs = np.asarray(X.sum(axis=0)).ravel()\n",
    "    terms = np.array(cv.get_feature_names_out())\n",
    "    top_idx = freqs.argsort()[::-1][:30]\n",
    "    top_terms = list(zip(terms[top_idx], freqs[top_idx]))\n",
    "    print('Top tokens:')\n",
    "    for t,f in top_terms[:30]:\n",
    "        print(f'{t}: {f}')\n",
    "else:\n",
    "    print('No text data for token frequency analysis.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039258fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def show_lsa_topics(texts, n_topics=6, n_terms=8):\n",
    "    tf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "    X = tf.fit_transform(texts)\n",
    "    svd = TruncatedSVD(n_components=n_topics, random_state=0)\n",
    "    svd.fit(X)\n",
    "    terms = tf.get_feature_names_out()\n",
    "    topics = []\n",
    "    for i, comp in enumerate(svd.components_):\n",
    "        top_terms = [terms[j] for j in comp.argsort()[-n_terms:][::-1]]\n",
    "        topics.append(top_terms)\n",
    "    for i,t in enumerate(topics):\n",
    "        print(f'Topic {i}:', ', '.join(t))\n",
    "\n",
    "if sample_df is not None and text_cols:\n",
    "    small_texts = sample_df[text_cols[0]].fillna('').astype(str).values[:2000]\n",
    "    show_lsa_topics(small_texts, n_topics=6, n_terms=8)\n",
    "else:\n",
    "    print('No text data for topic modeling.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d9df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "date_col = None\n",
    "for c in possible_date_cols:\n",
    "    try:\n",
    "        tmp = pd.to_datetime(sample_df[c], errors='coerce')\n",
    "        if tmp.notna().sum() > 0:\n",
    "            date_col = c\n",
    "            break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if date_col is not None:\n",
    "    print('Using date column:', date_col)\n",
    "    sample_df['_when'] = pd.to_datetime(sample_df[date_col], errors='coerce')\n",
    "    sample_df['_weekday'] = sample_df['_when'].dt.day_name()\n",
    "    sample_df['_hour'] = sample_df['_when'].dt.hour\n",
    "    # publisher column detection\n",
    "    publisher_cols = [c for c in sample_df.columns if 'publisher' in c.lower() or 'source' in c.lower()]\n",
    "    publisher = publisher_cols[0] if publisher_cols else None\n",
    "    # show counts by weekday and hour\n",
    "    wd = sample_df.groupby('_weekday').size().reindex(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']).fillna(0)\n",
    "    wd.plot(kind='bar');\n",
    "    plt.title('Counts by Weekday')\n",
    "    plt.show()\n",
    "    hr = sample_df.groupby('_hour').size()\n",
    "    hr.plot(kind='bar');\n",
    "    plt.title('Counts by Hour of Day')\n",
    "    plt.show()\n",
    "    if publisher is not None:\n",
    "        print('Using publisher column:', publisher)\n",
    "        top_publishers = sample_df[publisher].value_counts().head(10)\n",
    "        display(top_publishers)\n",
    "else:\n",
    "    print('No parsable date column found in sample; skipping time-based analysis.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
